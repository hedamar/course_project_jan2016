---
output: html_document
---
# Project for Machine Learning Class.
## Load libraries and the data. Then, choose the variables to be used in the prediction
```{r load_data, echo=TRUE, cache=TRUE}
library(caret)
library(gbm)

cp_training_data_raw <- read.csv("pml-training.csv", header = TRUE, sep = ",", quote = "\"", dec = ".", fill = TRUE, comment.char = "")
cp_testing_data_raw <- read.csv("pml-testing.csv", header = TRUE, sep = ",", quote = "\"", dec = ".", fill = TRUE, comment.char = "")
```

Visual inspection of the testing data (not shown here for brevity) reveals that a number of potential predictors are all "NA"s in the test data. These variables are also entirely (or almost entirely) missing from the training data. Therefore, I decided to remove these variables as potential predictors while training the model. In addition, I decided to remove the mutliple time stamp-related variables and the counter X from the set of predictors.

```{r choose_predictors, echo=TRUE, cache=TRUE}
cp_testing_data <- cp_testing_data_raw[, colSums(is.na(cp_testing_data_raw)) != nrow(cp_testing_data_raw)]
cp_testing_drop <- cp_testing_data_raw[, colSums(is.na(cp_testing_data_raw)) == nrow(cp_testing_data_raw)]
variables_drop <- names(cp_testing_drop)

cp_training_data <- cp_training_data_raw[ , -which(names(cp_training_data_raw) %in% variables_drop)]

cp_training_data <- cp_training_data[ , -which(names(cp_training_data) %in% c("X", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp"))]
```

All remaining variables are used as predictors in the model.

## Model Choice and Choice of Cross-Validation Metholodgy
I decided to use a random forest model, since it is highly accurate. As discussed in class, the main concerns with random forests are (a) speed, (b) interpretability and (c) overfitting. Interpretability is not an issue for this assignment, since we are not asked to interpret the results anyway. I took some steps to improve the speed (discussed below). Overfitting remains an issue, but overall, the model performed well on the test set, so I didn't see a point in going back and trying another model.

Regarding cross-validation, I decided to do K-fold cross-validation, where k = 10. This choice of K-fold CV was mostly driven by speed/computational power issues. The default CV for random forest in caret is repeated bootstrapping, which results in the model needing a lot of time to run. Finally, I set k = 10, since 3 or 5 seemed too low and a number of sources, including the Wikipedia page for K-fold cross-validation (https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation), state that k = 10 is very commonly used.

## Running the Model, Reporting the Accuracy and the Expected Out-of-Sample Error
Create the testing sample (70%) and the validation sample (30%).  The validation sample (which is called as such since we are provided with another "testing" sample with 20 observations) will be used for the out-of-sample error estimate.

```{r samples, echo=TRUE, cache=TRUE}
val <- createDataPartition(cp_training_data$classe, p=0.7, list=FALSE )
cp_final_training <- cp_training_data[val,]
cp_validate <- cp_training_data[-val,]
```

```{r run_model, echo=TRUE, cache=TRUE}
modfit_rf <- train(classe ~., method="rf", trControl=trainControl(method = "cv", number = 10, savePredictions = TRUE), data=cp_final_training, na.action=na.omit)

print(modfit_rf)
print(modfit_rf$finalModel)
```

The output suggests that the model is very accurate within the training sample. It has selected a model that uses 30 random predictors at each split (when the variables are bootstrapped). The overall accuracy is 0.9976, which can be confirmed by the confusion matrix and the OOB estimate of  error rate.

Regarding the expected out-of-sample error, use the validation sample to calculate the out-of-sample error/accuracy:

```{r validate, echo=TRUE, cache=TRUE}
validate_rf <- predict(modfit_rf, cp_validate)

table(validate_rf, cp_validate$classe)
```

The model has very high accuracy in predicting the validation sample. The table implies an accuracy of 99.78 and hence an out-of-sample error of 0.22%. Also, since the OOB estimate of error rate was 0.24% during the training stage of the model, the expected out-of-sample error is between 0.22% and 0.24%. The high accuracy of the model can be due to random forest overfitting the model (or the presence of predictor that is almost perfectly correlated with the outcome).

Finally, the model is used to predict the outcomes for the test sample that was provided.

```{r predict, echo=TRUE, cache=TRUE}
quiz_responses <- predict(modfit_rf, cp_testing_data)
```

