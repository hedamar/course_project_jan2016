---
output: html_document
---
# Project for Machine Learning Class.
## Load libraries and the data. Then, choose the variables to be used in the prediction
```{r load_data, echo=TRUE, cache=TRUE}
library(caret)
library(gbm)

cp_training_data_raw <- read.csv("pml-training.csv", header = TRUE, sep = ",", quote = "\"", dec = ".", fill = TRUE, comment.char = "")
cp_testing_data_raw <- read.csv("pml-testing.csv", header = TRUE, sep = ",", quote = "\"", dec = ".", fill = TRUE, comment.char = "")
```

Visual inspection of the testing data (not shown here for brevity) reveals that a number of potential predictors are all "NA"s in the test data. These variables are also entirely (or almost entirely) missing from the training data. Therefore, I decided to remove these variables as potential predictors while training the model. In addition, I decided to remove the mutliple time stamp-related variables and the counter X from the set of predictors.

```{r choose_predictors, echo=TRUE, cache=TRUE}
cp_testing_data <- cp_testing_data_raw[, colSums(is.na(cp_testing_data_raw)) != nrow(cp_testing_data_raw)]
cp_testing_drop <- cp_testing_data_raw[, colSums(is.na(cp_testing_data_raw)) == nrow(cp_testing_data_raw)]
variables_drop <- names(cp_testing_drop)

cp_training_data <- cp_training_data_raw[ , -which(names(cp_training_data_raw) %in% variables_drop)]

cp_training_data <- cp_training_data[ , -which(names(cp_training_data) %in% c("X", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp"))]
```

All remaining variables are used as predictors in the model.

## Model Choice and Choice of Cross-Validation Metholodgy
I decided to use a random forest model, since it is highly accurate. As discussed in class, the main concerns with random forests are (a) speed, (b) interpretability and (c) overfitting. Interpretability is not an issue for this assignment, since we are not asked to interpret the results anyway. I took some steps to improve the speed (discussed below). Overfitting remains an issue, but overall, the model performed well on the test set, so I didn't see a point in going back and trying another model.

Regarding cross-validation, I decided to do K-fold cross-validation, where k = 10. This choice of K-fold CV was mostly driven by speed/computational power issues. The default CV for random forest in caret is repeated bootstrapping, which results in the model needing a lot of time to run. Finally, I set k = 10, since 3 or 5 seemed too low and a number of sources, including the Wikipedia page for K-fold cross-validation (https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation), state that k = 10 is very commonly used.

## Running the Model, Reporting the Accuracy and the Expected Out-of-Sample Error

```{r run_model, echo=TRUE, cache=TRUE}
modfit_rf <- train(classe ~., method="rf", trControl=trainControl(method = "cv", number = 10, savePredictions = TRUE), data=cp_training_data, na.action=na.omit)

print(modfit_rf)
print(modfit_rf$finalModel)
```

The output suggests that the model is very accurate. It has selected a model that uses 30 random predictors at each split (when the variables are bootstrapped). The overall accuracy is 0.9985, which can be confirmed by the confusion matrix and the OOB estimate of  error rate.

Regarding the expected out-of-sample error, the confusion matrix reports the OOB esimtate of the error rate to be 0.15%. Since the random forest method calculates each tree by bootstrap where a portion of the sample is left out, I interpreted the OOB estimate of  error rate as the expected out-of-sample error.

Finally, the model is used to predict the outcomes for the test sample.

```{r predict, echo=TRUE, cache=TRUE}
quiz_responses <- predict(modfit_rf, cp_testing_data)
```

